<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Stack Overflow Analysis on CSS Final Project Group 21</title><link>https://s204151.github.io/CSS_webpage/</link><description>Recent content in Stack Overflow Analysis on CSS Final Project Group 21</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://s204151.github.io/CSS_webpage/index.xml" rel="self" type="application/rss+xml"/><item><title>Data description</title><link>https://s204151.github.io/CSS_webpage/data-description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://s204151.github.io/CSS_webpage/data-description/</guid><description>The dataset that has been used for this project is named StackSample. It contains 10% of stack overflows questions, answers and tags data.
The datasets are found on the website: https://www.kaggle.com/datasets/stackoverflow/stacksample
All the datasets used in this project can be downloaded from here: https://drive.google.com/drive/folders/1RdU0mMRiBL9uTIWgPd8ZTfrs3qKUc8qB?usp=sharing
The files that can be downloaded from the website are:
Questions.csv - 1.79 Gb Answers.csv - 1.5 Gb Tags.csv - 65 Mb There is only need for the Questions and Answers files for this project.</description></item><item><title>Network analysis</title><link>https://s204151.github.io/CSS_webpage/network-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://s204151.github.io/CSS_webpage/network-analysis/</guid><description>The directed-graph is the first thing created, nodes are users and a directed connection is when a person answers another persons question (also weighted such that multiple answers from one person to another are counted).
The communities are detected using Louivain-method, and visualized with netwulf: In the image, it is shown that 1079 communities has been created where each represents a color. On the outskirt of the image there are also nodes but without edges, this must mean that they are questions which hasn&amp;rsquo;t been answered, but Louivains-method was still able to categorize them into communities.</description></item><item><title>Text analysis</title><link>https://s204151.github.io/CSS_webpage/text-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://s204151.github.io/CSS_webpage/text-analysis/</guid><description>Throughout the dataset, question body text, question title text and answer body text is tokenized and stopwords are removed. Bi-grams are also found for semi-contextual visualization of discussed topics in different communities using WordClouds.
To measure importance of terms, TF-IDF is used. The (non-normalized) Term Frequency is used,
$$ \text{TF}(t,d) = f_{t,d} $$
Where f_{t,d} is simply the term count in document. The Inverse Document Frequency is calculated as
where $n_t = |{d \in D: t \in d}|$ is the number of documents $D$ where the term $t$ appears at least once.</description></item></channel></rss>